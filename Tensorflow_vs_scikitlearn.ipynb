{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\n#sample_submission = pd.read_csv(\"../input/digit-recognizer/sample_submission.csv\")\n#test = pd.read_csv(\"../input/digit-recognizer/test.csv\")\ntrain = pd.read_csv(\"../input/digit-recognizer/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(train.iloc[0:41500 , 1:])\ny = np.array(train.iloc[0:41500, :1]).ravel()\n\nprint(X.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Dropout , BatchNormalization\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\n\n\n\n\ndef create_network():\n    network = tf.keras.Sequential()\n    network.add(layers.Dense(300,activation='elu', input_shape=(784,)))\n    network.add(layers.BatchNormalization())\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(100,activation='elu'))\n    network.add(layers.BatchNormalization())\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(10, activation='softmax'))\n    # Compile model\n    network.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n    network.summary()\n    \n    return network\n\nneural_network = KerasClassifier(build_fn=create_network, \n                                 epochs=50, \n                                 batch_size=200, \n                                 verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate , StratifiedShuffleSplit, ShuffleSplit\nimport matplotlib.pyplot as plt\n\n#numeric_transformer=Pipeline(steps=['scaler',StandardScaler()])\nscaler=StandardScaler()\nclassifier=MLPClassifier(hidden_layer_sizes=(300,100), activation='relu', solver='adam', alpha=0.0001, batch_size=200, \n                         learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=50, shuffle=True, \n                         random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n                         nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n                         epsilon=1e-08, n_iter_no_change=10)\n\nscikit_clf=Pipeline(steps=[('scaler',scaler),\n                   ('classifier', classifier)])\n\ntensor_clf=Pipeline(steps=[('scaler',scaler),\n                   ('classifier', neural_network)])\n\n\ndef apply_cv(X, y, nFold, test_size, shuffled):   \n    metrics = ['accuracy']\n    if shuffled:\n        rs = ShuffleSplit(n_splits=nFold, test_size=test_size, random_state=10)\n        outcomes=cross_validate(mlp, X, y, scoring=metrics, cv=rs, return_train_score=False)\n    else:\n        kf = KFold(n_splits=nFold)\n        outcomes=cross_validate(mlp, X, y, scoring=metrics, cv=kf, return_train_score=False)\n    return outcomes['test_accuracy']\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparativo com dados originais"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold=5\nkf = KFold(n_splits=n_fold)\n\n\ntensor_original = cross_val_score(tensor_clf, X, y, cv=kf)\n\n\nscikit_original = cross_val_score(scikit_clf, X, y, cv=kf)\n\n\n\n\nplt.title('Accuracy - scikit-learn & Tensorflow 41.500 instâncias')\nplt.grid(color='silver', linestyle='-', linewidth=1, alpha=0.8)\nplt.scatter(range(1,n_fold+1),scikit_original,color='blue',alpha=1)\nplt.scatter(range(1,n_fold+1),tensor_original,color='orange',alpha=1)\nplt.legend(('scikit-learn','TensorFlow'), scatterpoints =1, loc='lower left',ncol=1, fontsize=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Os modelos estão Aprendendo?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n    \n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator=scikit_clf  # pode ser qualquer estimador\nplot_learning_curve(estimator, title, X_train, y_train, (0.7, 1.01), cv=cv, n_jobs=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator=tensor_clf  # pode ser qualquer estimador\nplot_learning_curve(estimator, title, X_train, y_train, (0.7, 1.01), cv=cv, n_jobs=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Criando dados sintéticos"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.ndimage.interpolation import shift\ndef shift_image(image, dx, dy):\n    image = image.reshape((28, 28))\n    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n    return shifted_image.reshape([-1])\n\nX_train_augmented = [image for image in X]\ny_train_augmented = [label for label in y]\n\nfor dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):   #variação de 1 pixels em todas as direções\n    for image, label in zip(X, y):\n        X_train_augmented.append(shift_image(image, dx, dy))\n        y_train_augmented.append(label)\n\nX_train = np.array(X_train_augmented)\ny_train = np.array(y_train_augmented)\n\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treinando com dados sintéticos"},{"metadata":{"trusted":true},"cell_type":"code","source":"tensor_augmented = cross_val_score(tensor_clf, X_train, y_train, cv=kf)\nmlp_augmented = cross_val_score(scikit_clf, X_train, y_train, cv=kf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gráfico geral"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Accuracy - scikit-learn & Tensorflow 207.500 instâncias')\nplt.grid(color='silver', linestyle='-', linewidth=1, alpha=0.8)\nplt.scatter(range(1,n_fold+1),mlp_augmented,color='blue',alpha=1.0)\nplt.scatter(range(1,n_fold+1),tensor_augmented,color='orange',alpha=1.0)\nplt.scatter(range(1,n_fold+1),tensor_original,color='orange',alpha=0.3)\nplt.scatter(range(1,n_fold+1),mlp_original,color='blue',alpha=0.3)\nplt.legend(('scikit-learn','TensorFlow'), scatterpoints =1, loc='lower left',ncol=1, fontsize=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}